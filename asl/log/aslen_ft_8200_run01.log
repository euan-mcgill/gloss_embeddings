[2024-03-11 13:12:04,552 WARNING] Corpus corpus_1's weight should be given. We default it to 1 for you.
[2024-03-11 13:12:04,552 INFO] Parsed 2 corpora from -data.
[2024-03-11 13:12:04,552 INFO] Loading checkpoint from gt-pretrain/models/gt-pretrain_01_step_8200.pt
[2024-03-11 13:12:04,645 INFO] Building model...
[2024-03-11 13:12:04,706 INFO] Switching model to float32 for amp/apex_amp
[2024-03-11 13:12:04,706 INFO] Non quantized layer compute is fp32
[2024-03-11 13:12:06,475 INFO] NMTModel(
  (encoder): RNNEncoder(
    (embeddings): Embeddings(
      (make_embedding): Sequential(
        (emb_luts): Elementwise(
          (0): Embedding(14144, 500, padding_idx=1)
        )
      )
      (dropout): Dropout(p=0.3, inplace=False)
    )
    (rnn): LSTM(500, 500, num_layers=2, batch_first=True, dropout=0.3)
  )
  (decoder): InputFeedRNNDecoder(
    (embeddings): Embeddings(
      (make_embedding): Sequential(
        (emb_luts): Elementwise(
          (0): Embedding(17816, 500, padding_idx=1)
        )
      )
      (dropout): Dropout(p=0.3, inplace=False)
    )
    (dropout): Dropout(p=0.3, inplace=False)
    (rnn): StackedLSTM(
      (dropout): Dropout(p=0.3, inplace=False)
      (layers): ModuleList(
        (0): LSTMCell(1000, 500)
        (1): LSTMCell(500, 500)
      )
    )
    (attn): GlobalAttention(
      (linear_in): Linear(in_features=500, out_features=500, bias=False)
      (linear_out): Linear(in_features=1000, out_features=500, bias=False)
    )
  )
  (generator): Linear(in_features=500, out_features=17816, bias=True)
)
[2024-03-11 13:12:06,476 INFO] encoder: 11080000
[2024-03-11 13:12:06,476 INFO] decoder: 23591816
[2024-03-11 13:12:06,476 INFO] * number of parameters: 34671816
[2024-03-11 13:12:06,476 INFO] Trainable parameters = {'torch.float32': 34671816, 'torch.float16': 0, 'torch.uint8': 0, 'torch.int8': 0}
[2024-03-11 13:12:06,476 INFO] Non trainable parameters = {'torch.float32': 0, 'torch.float16': 0, 'torch.uint8': 0, 'torch.int8': 0}
[2024-03-11 13:12:06,476 INFO]  * src vocab size = 14144
[2024-03-11 13:12:06,476 INFO]  * tgt vocab size = 17816
[2024-03-11 13:12:06,478 INFO] Starting training on GPU: [0]
[2024-03-11 13:12:06,478 INFO] Start training loop and validate every 200 steps...
[2024-03-11 13:12:06,478 INFO] Scoring with: TransformPipe(InferFeatsTransform())
[2024-03-11 13:40:52,771 WARNING] Corpus corpus_1's weight should be given. We default it to 1 for you.
[2024-03-11 13:40:52,771 INFO] Parsed 2 corpora from -data.
[2024-03-11 13:40:52,771 INFO] Loading checkpoint from gt-pretrain/models/gt-pretrain_01_step_8200.pt
[2024-03-11 13:40:52,853 INFO] Building model...
[2024-03-11 13:40:52,912 INFO] Switching model to float32 for amp/apex_amp
[2024-03-11 13:40:52,912 INFO] Non quantized layer compute is fp32
[2024-03-11 13:40:54,659 INFO] NMTModel(
  (encoder): RNNEncoder(
    (embeddings): Embeddings(
      (make_embedding): Sequential(
        (emb_luts): Elementwise(
          (0): Embedding(14144, 500, padding_idx=1)
        )
      )
      (dropout): Dropout(p=0.3, inplace=False)
    )
    (rnn): LSTM(500, 500, num_layers=2, batch_first=True, dropout=0.3)
  )
  (decoder): InputFeedRNNDecoder(
    (embeddings): Embeddings(
      (make_embedding): Sequential(
        (emb_luts): Elementwise(
          (0): Embedding(17816, 500, padding_idx=1)
        )
      )
      (dropout): Dropout(p=0.3, inplace=False)
    )
    (dropout): Dropout(p=0.3, inplace=False)
    (rnn): StackedLSTM(
      (dropout): Dropout(p=0.3, inplace=False)
      (layers): ModuleList(
        (0): LSTMCell(1000, 500)
        (1): LSTMCell(500, 500)
      )
    )
    (attn): GlobalAttention(
      (linear_in): Linear(in_features=500, out_features=500, bias=False)
      (linear_out): Linear(in_features=1000, out_features=500, bias=False)
    )
  )
  (generator): Linear(in_features=500, out_features=17816, bias=True)
)
[2024-03-11 13:40:54,660 INFO] encoder: 11080000
[2024-03-11 13:40:54,660 INFO] decoder: 23591816
[2024-03-11 13:40:54,660 INFO] * number of parameters: 34671816
[2024-03-11 13:40:54,660 INFO] Trainable parameters = {'torch.float32': 34671816, 'torch.float16': 0, 'torch.uint8': 0, 'torch.int8': 0}
[2024-03-11 13:40:54,660 INFO] Non trainable parameters = {'torch.float32': 0, 'torch.float16': 0, 'torch.uint8': 0, 'torch.int8': 0}
[2024-03-11 13:40:54,660 INFO]  * src vocab size = 14144
[2024-03-11 13:40:54,660 INFO]  * tgt vocab size = 17816
[2024-03-11 13:40:54,662 INFO] Starting training on GPU: [0]
[2024-03-11 13:40:54,662 INFO] Start training loop and validate every 200 steps...
[2024-03-11 13:40:54,662 INFO] Scoring with: TransformPipe(InferFeatsTransform())
[2024-03-11 13:41:27,711 WARNING] Corpus corpus_1's weight should be given. We default it to 1 for you.
[2024-03-11 13:41:27,711 INFO] Parsed 2 corpora from -data.
[2024-03-11 13:41:27,711 INFO] Loading checkpoint from gt-pretrain/models/gt-pretrain_01_step_8200.pt
[2024-03-11 13:41:27,791 INFO] Building model...
[2024-03-11 13:41:27,850 INFO] Switching model to float32 for amp/apex_amp
[2024-03-11 13:41:27,850 INFO] Non quantized layer compute is fp32
[2024-03-11 13:41:29,615 INFO] NMTModel(
  (encoder): RNNEncoder(
    (embeddings): Embeddings(
      (make_embedding): Sequential(
        (emb_luts): Elementwise(
          (0): Embedding(14144, 500, padding_idx=1)
        )
      )
      (dropout): Dropout(p=0.3, inplace=False)
    )
    (rnn): LSTM(500, 500, num_layers=2, batch_first=True, dropout=0.3)
  )
  (decoder): InputFeedRNNDecoder(
    (embeddings): Embeddings(
      (make_embedding): Sequential(
        (emb_luts): Elementwise(
          (0): Embedding(17816, 500, padding_idx=1)
        )
      )
      (dropout): Dropout(p=0.3, inplace=False)
    )
    (dropout): Dropout(p=0.3, inplace=False)
    (rnn): StackedLSTM(
      (dropout): Dropout(p=0.3, inplace=False)
      (layers): ModuleList(
        (0): LSTMCell(1000, 500)
        (1): LSTMCell(500, 500)
      )
    )
    (attn): GlobalAttention(
      (linear_in): Linear(in_features=500, out_features=500, bias=False)
      (linear_out): Linear(in_features=1000, out_features=500, bias=False)
    )
  )
  (generator): Linear(in_features=500, out_features=17816, bias=True)
)
[2024-03-11 13:41:29,615 INFO] encoder: 11080000
[2024-03-11 13:41:29,615 INFO] decoder: 23591816
[2024-03-11 13:41:29,615 INFO] * number of parameters: 34671816
[2024-03-11 13:41:29,615 INFO] Trainable parameters = {'torch.float32': 34671816, 'torch.float16': 0, 'torch.uint8': 0, 'torch.int8': 0}
[2024-03-11 13:41:29,615 INFO] Non trainable parameters = {'torch.float32': 0, 'torch.float16': 0, 'torch.uint8': 0, 'torch.int8': 0}
[2024-03-11 13:41:29,615 INFO]  * src vocab size = 14144
[2024-03-11 13:41:29,615 INFO]  * tgt vocab size = 17816
[2024-03-11 13:41:29,617 INFO] Starting training on GPU: [0]
[2024-03-11 13:41:29,617 INFO] Start training loop and validate every 200 steps...
[2024-03-11 13:41:29,617 INFO] Scoring with: TransformPipe(InferFeatsTransform())
[2024-03-11 13:41:35,521 INFO] Step 8250/13200; acc: 20.8; ppl: 348.4; xent: 5.9; lr: 1.00000; sents:    2393; bsz:  448/ 625/48; 3797/5294 tok/s;      6 sec;
[2024-03-11 13:41:38,050 INFO] Step 8300/13200; acc: 28.6; ppl:  81.0; xent: 4.4; lr: 1.00000; sents:    2296; bsz:  448/ 611/46; 8856/12084 tok/s;      8 sec;
[2024-03-11 13:41:40,527 INFO] Step 8350/13200; acc: 34.5; ppl:  43.6; xent: 3.8; lr: 1.00000; sents:    2391; bsz:  464/ 635/48; 9371/12817 tok/s;     11 sec;
[2024-03-11 13:41:42,985 INFO] Step 8400/13200; acc: 38.7; ppl:  28.5; xent: 3.4; lr: 1.00000; sents:    2279; bsz:  435/ 599/46; 8850/12179 tok/s;     13 sec;
[2024-03-11 13:41:46,523 INFO] valid stats calculation
                           took: 3.5369985103607178 s.
[2024-03-11 13:41:46,524 INFO] Train perplexity: 77.5826
[2024-03-11 13:41:46,524 INFO] Train accuracy: 30.6025
[2024-03-11 13:41:46,524 INFO] Sentences processed: 9359
[2024-03-11 13:41:46,524 INFO] Average bsz:  449/ 617/47
[2024-03-11 13:41:46,524 INFO] Validation perplexity: 69.8063
[2024-03-11 13:41:46,524 INFO] Validation accuracy: 30.8113
[2024-03-11 13:41:46,526 INFO] Saving checkpoint gt-finetune/gt-finetune_8200_01_step_8400.pt
[2024-03-11 13:41:49,234 INFO] Step 8450/13200; acc: 43.4; ppl:  19.7; xent: 3.0; lr: 1.00000; sents:    2504; bsz:  468/ 651/50; 3747/5209 tok/s;     20 sec;
[2024-03-11 13:41:52,737 INFO] Step 8500/13200; acc: 46.8; ppl:  14.7; xent: 2.7; lr: 1.00000; sents:    2288; bsz:  438/ 603/46; 6249/8609 tok/s;     23 sec;
[2024-03-11 13:41:56,488 INFO] Step 8550/13200; acc: 49.4; ppl:  11.7; xent: 2.5; lr: 1.00000; sents:    2302; bsz:  446/ 609/46; 5944/8119 tok/s;     27 sec;
[2024-03-11 13:42:00,217 INFO] Step 8600/13200; acc: 53.1; ppl:   9.2; xent: 2.2; lr: 1.00000; sents:    2509; bsz:  465/ 651/50; 6241/8733 tok/s;     31 sec;
[2024-03-11 13:42:03,704 INFO] valid stats calculation
                           took: 3.48714017868042 s.
[2024-03-11 13:42:03,705 INFO] Train perplexity: 31.8751
[2024-03-11 13:42:03,705 INFO] Train accuracy: 39.4608
[2024-03-11 13:42:03,705 INFO] Sentences processed: 18962
[2024-03-11 13:42:03,705 INFO] Average bsz:  452/ 623/47
[2024-03-11 13:42:03,705 INFO] Validation perplexity: 61.0478
[2024-03-11 13:42:03,705 INFO] Validation accuracy: 35.2865
[2024-03-11 13:42:03,706 INFO] Saving checkpoint gt-finetune/gt-finetune_8200_01_step_8600.pt
[2024-03-11 13:42:06,407 INFO] Step 8650/13200; acc: 55.8; ppl:   7.6; xent: 2.0; lr: 1.00000; sents:    2301; bsz:  445/ 609/46; 3599/4917 tok/s;     37 sec;
[2024-03-11 13:42:10,215 INFO] Step 8700/13200; acc: 58.1; ppl:   6.5; xent: 1.9; lr: 1.00000; sents:    2399; bsz:  455/ 629/48; 5974/8252 tok/s;     41 sec;
[2024-03-11 13:42:13,853 INFO] Step 8750/13200; acc: 61.3; ppl:   5.4; xent: 1.7; lr: 1.00000; sents:    2285; bsz:  424/ 590/46; 5832/8107 tok/s;     44 sec;
[2024-03-11 13:42:17,684 INFO] Step 8800/13200; acc: 63.1; ppl:   4.9; xent: 1.6; lr: 1.00000; sents:    2392; bsz:  455/ 628/48; 5945/8193 tok/s;     48 sec;
[2024-03-11 13:42:21,299 INFO] valid stats calculation
                           took: 3.6147994995117188 s.
[2024-03-11 13:42:21,300 INFO] Train perplexity: 18.3528
[2024-03-11 13:42:21,300 INFO] Train accuracy: 46.1018
[2024-03-11 13:42:21,300 INFO] Sentences processed: 28339
[2024-03-11 13:42:21,300 INFO] Average bsz:  449/ 620/47
[2024-03-11 13:42:21,300 INFO] Validation perplexity: 76.2965
[2024-03-11 13:42:21,300 INFO] Validation accuracy: 37.5402
[2024-03-11 13:42:21,301 INFO] Saving checkpoint gt-finetune/gt-finetune_8200_01_step_8800.pt
[2024-03-11 13:42:24,180 INFO] Step 8850/13200; acc: 65.3; ppl:   4.3; xent: 1.5; lr: 1.00000; sents:    2393; bsz:  450/ 623/48; 3465/4793 tok/s;     55 sec;
[2024-03-11 13:42:28,052 INFO] Step 8900/13200; acc: 66.4; ppl:   3.9; xent: 1.4; lr: 1.00000; sents:    2295; bsz:  469/ 628/46; 6062/8111 tok/s;     58 sec;
[2024-03-11 13:42:32,009 INFO] Step 8950/13200; acc: 69.3; ppl:   3.5; xent: 1.2; lr: 1.00000; sents:    2391; bsz:  449/ 623/48; 5679/7867 tok/s;     62 sec;
[2024-03-11 13:42:35,783 INFO] Step 9000/13200; acc: 71.3; ppl:   3.1; xent: 1.1; lr: 1.00000; sents:    2290; bsz:  439/ 602/46; 5816/7979 tok/s;     66 sec;
[2024-03-11 13:42:39,282 INFO] valid stats calculation
                           took: 3.498971462249756 s.
[2024-03-11 13:42:39,282 INFO] Train perplexity: 12.2803
[2024-03-11 13:42:39,282 INFO] Train accuracy: 51.5795
[2024-03-11 13:42:39,283 INFO] Sentences processed: 37708
[2024-03-11 13:42:39,283 INFO] Average bsz:  450/ 620/47
[2024-03-11 13:42:39,283 INFO] Validation perplexity: 80.5418
[2024-03-11 13:42:39,283 INFO] Validation accuracy: 38.0876
[2024-03-11 13:42:39,284 INFO] Saving checkpoint gt-finetune/gt-finetune_8200_01_step_9000.pt
[2024-03-11 13:42:42,404 INFO] Step 9050/13200; acc: 73.1; ppl:   2.8; xent: 1.0; lr: 1.00000; sents:    2500; bsz:  454/ 639/50; 3428/4823 tok/s;     73 sec;
[2024-03-11 13:42:46,186 INFO] Step 9100/13200; acc: 74.3; ppl:   2.7; xent: 1.0; lr: 1.00000; sents:    2299; bsz:  443/ 606/46; 5859/8014 tok/s;     77 sec;
[2024-03-11 13:42:49,947 INFO] Step 9150/13200; acc: 75.5; ppl:   2.5; xent: 0.9; lr: 1.00000; sents:    2401; bsz:  459/ 634/48; 6096/8429 tok/s;     80 sec;
[2024-03-11 13:42:53,612 INFO] Step 9200/13200; acc: 77.6; ppl:   2.3; xent: 0.8; lr: 1.00000; sents:    2305; bsz:  449/ 609/46; 6127/8305 tok/s;     84 sec;
[2024-03-11 13:42:57,150 INFO] valid stats calculation
                           took: 3.538144826889038 s.
[2024-03-11 13:42:57,151 INFO] Train perplexity: 8.97326
[2024-03-11 13:42:57,151 INFO] Train accuracy: 56.2947
[2024-03-11 13:42:57,151 INFO] Sentences processed: 47213
[2024-03-11 13:42:57,151 INFO] Average bsz:  450/ 620/47
[2024-03-11 13:42:57,151 INFO] Validation perplexity: 101.173
[2024-03-11 13:42:57,151 INFO] Validation accuracy: 38.3129
[2024-03-11 13:42:57,152 INFO] Saving checkpoint gt-finetune/gt-finetune_8200_01_step_9200.pt
[2024-03-11 13:43:00,256 INFO] Step 9250/13200; acc: 78.7; ppl:   2.2; xent: 0.8; lr: 1.00000; sents:    2401; bsz:  443/ 618/48; 3333/4654 tok/s;     91 sec;
[2024-03-11 13:43:04,049 INFO] Step 9300/13200; acc: 80.3; ppl:   2.1; xent: 0.7; lr: 1.00000; sents:    2303; bsz:  443/ 610/46; 5843/8047 tok/s;     94 sec;
[2024-03-11 13:43:07,779 INFO] Step 9350/13200; acc: 82.0; ppl:   1.9; xent: 0.7; lr: 1.00000; sents:    2395; bsz:  442/ 616/48; 5928/8261 tok/s;     98 sec;
[2024-03-11 13:43:11,555 INFO] Step 9400/13200; acc: 82.6; ppl:   1.9; xent: 0.6; lr: 1.00000; sents:    2393; bsz:  454/ 629/48; 6013/8326 tok/s;    102 sec;
[2024-03-11 13:43:15,079 INFO] valid stats calculation
                           took: 3.5235955715179443 s.
[2024-03-11 13:43:15,080 INFO] Train perplexity: 6.99005
[2024-03-11 13:43:15,080 INFO] Train accuracy: 60.3841
[2024-03-11 13:43:15,080 INFO] Sentences processed: 56705
[2024-03-11 13:43:15,080 INFO] Average bsz:  449/ 620/47
[2024-03-11 13:43:15,080 INFO] Validation perplexity: 128.957
[2024-03-11 13:43:15,080 INFO] Validation accuracy: 38.6671
[2024-03-11 13:43:15,081 INFO] Saving checkpoint gt-finetune/gt-finetune_8200_01_step_9400.pt
[2024-03-11 13:43:18,278 INFO] Step 9450/13200; acc: 83.4; ppl:   1.8; xent: 0.6; lr: 1.00000; sents:    2392; bsz:  444/ 617/48; 3303/4590 tok/s;    109 sec;
[2024-03-11 13:43:22,154 INFO] Step 9500/13200; acc: 83.4; ppl:   1.8; xent: 0.6; lr: 1.00000; sents:    2295; bsz:  463/ 629/46; 5972/8118 tok/s;    113 sec;
[2024-03-11 13:43:25,968 INFO] Step 9550/13200; acc: 85.0; ppl:   1.7; xent: 0.5; lr: 1.00000; sents:    2391; bsz:  444/ 618/48; 5821/8101 tok/s;    116 sec;
[2024-03-11 13:43:29,759 INFO] Step 9600/13200; acc: 85.9; ppl:   1.6; xent: 0.5; lr: 1.00000; sents:    2391; bsz:  442/ 615/48; 5826/8110 tok/s;    120 sec;
[2024-03-11 13:43:33,315 INFO] valid stats calculation
                           took: 3.5562984943389893 s.
[2024-03-11 13:43:33,316 INFO] Train perplexity: 5.72566
[2024-03-11 13:43:33,316 INFO] Train accuracy: 63.8197
[2024-03-11 13:43:33,316 INFO] Sentences processed: 66174
[2024-03-11 13:43:33,316 INFO] Average bsz:  449/ 620/47
[2024-03-11 13:43:33,316 INFO] Validation perplexity: 126.955
[2024-03-11 13:43:33,316 INFO] Validation accuracy: 40.3735
[2024-03-11 13:43:33,317 INFO] Saving checkpoint gt-finetune/gt-finetune_8200_01_step_9600.pt
[2024-03-11 13:43:36,958 INFO] Step 9650/13200; acc: 86.1; ppl:   1.6; xent: 0.5; lr: 1.00000; sents:    2303; bsz:  456/ 628/46; 3168/4362 tok/s;    127 sec;
[2024-03-11 13:43:40,817 INFO] Step 9700/13200; acc: 87.0; ppl:   1.6; xent: 0.5; lr: 1.00000; sents:    2399; bsz:  444/ 617/48; 5747/7996 tok/s;    131 sec;
[2024-03-11 13:43:44,541 INFO] Step 9750/13200; acc: 87.8; ppl:   1.5; xent: 0.4; lr: 1.00000; sents:    2401; bsz:  446/ 620/48; 5985/8320 tok/s;    135 sec;
[2024-03-11 13:43:48,277 INFO] Step 9800/13200; acc: 88.3; ppl:   1.5; xent: 0.4; lr: 1.00000; sents:    2307; bsz:  445/ 610/46; 5957/8159 tok/s;    139 sec;
[2024-03-11 13:43:51,861 INFO] valid stats calculation
                           took: 3.5839810371398926 s.
[2024-03-11 13:43:51,861 INFO] Train perplexity: 4.8641
[2024-03-11 13:43:51,862 INFO] Train accuracy: 66.7498
[2024-03-11 13:43:51,862 INFO] Sentences processed: 75584
[2024-03-11 13:43:51,862 INFO] Average bsz:  449/ 620/47
[2024-03-11 13:43:51,862 INFO] Validation perplexity: 158.371
[2024-03-11 13:43:51,862 INFO] Validation accuracy: 39.9549
[2024-03-11 13:43:51,863 INFO] Saving checkpoint gt-finetune/gt-finetune_8200_01_step_9800.pt
[2024-03-11 13:43:55,575 INFO] Step 9850/13200; acc: 88.7; ppl:   1.5; xent: 0.4; lr: 1.00000; sents:    2400; bsz:  455/ 629/48; 3119/4312 tok/s;    146 sec;
[2024-03-11 13:43:59,331 INFO] Step 9900/13200; acc: 89.4; ppl:   1.4; xent: 0.4; lr: 1.00000; sents:    2399; bsz:  446/ 624/48; 5937/8305 tok/s;    150 sec;
[2024-03-11 13:44:03,173 INFO] Step 9950/13200; acc: 90.0; ppl:   1.4; xent: 0.3; lr: 1.00000; sents:    2298; bsz:  448/ 610/46; 5830/7938 tok/s;    154 sec;
[2024-03-11 13:44:07,086 INFO] Step 10000/13200; acc: 89.8; ppl:   1.4; xent: 0.3; lr: 1.00000; sents:    2393; bsz:  468/ 639/48; 5979/8166 tok/s;    157 sec;
[2024-03-11 13:44:10,704 INFO] valid stats calculation
                           took: 3.6172592639923096 s.
[2024-03-11 13:44:10,704 INFO] Train perplexity: 4.24141
[2024-03-11 13:44:10,704 INFO] Train accuracy: 69.298
[2024-03-11 13:44:10,704 INFO] Sentences processed: 85074
[2024-03-11 13:44:10,705 INFO] Average bsz:  450/ 620/47
[2024-03-11 13:44:10,705 INFO] Validation perplexity: 192.542
[2024-03-11 13:44:10,705 INFO] Validation accuracy: 39.7939
[2024-03-11 13:44:10,706 INFO] Saving checkpoint gt-finetune/gt-finetune_8200_01_step_10000.pt
[2024-03-11 13:44:14,570 INFO] Step 10050/13200; acc: 90.3; ppl:   1.4; xent: 0.3; lr: 1.00000; sents:    2280; bsz:  438/ 602/46; 2928/4020 tok/s;    165 sec;
[2024-03-11 13:44:18,488 INFO] Step 10100/13200; acc: 90.8; ppl:   1.4; xent: 0.3; lr: 1.00000; sents:    2391; bsz:  462/ 633/48; 5899/8078 tok/s;    169 sec;
[2024-03-11 13:44:22,298 INFO] Step 10150/13200; acc: 91.0; ppl:   1.3; xent: 0.3; lr: 1.00000; sents:    2391; bsz:  441/ 617/48; 5784/8102 tok/s;    173 sec;
[2024-03-11 13:44:26,047 INFO] Step 10200/13200; acc: 91.3; ppl:   1.3; xent: 0.3; lr: 1.00000; sents:    2286; bsz:  439/ 602/46; 5853/8023 tok/s;    176 sec;
[2024-03-11 13:44:29,669 INFO] valid stats calculation
                           took: 3.621694326400757 s.
[2024-03-11 13:44:29,670 INFO] Train perplexity: 3.7906
[2024-03-11 13:44:29,670 INFO] Train accuracy: 71.4328
[2024-03-11 13:44:29,670 INFO] Sentences processed: 94422
[2024-03-11 13:44:29,670 INFO] Average bsz:  449/ 620/47
[2024-03-11 13:44:29,670 INFO] Validation perplexity: 188.228
[2024-03-11 13:44:29,670 INFO] Validation accuracy: 39.5042
[2024-03-11 13:44:29,671 INFO] Saving checkpoint gt-finetune/gt-finetune_8200_01_step_10200.pt
[2024-03-11 13:44:33,735 INFO] Step 10250/13200; acc: 91.7; ppl:   1.3; xent: 0.3; lr: 1.00000; sents:    2404; bsz:  461/ 636/48; 2998/4137 tok/s;    184 sec;
[2024-03-11 13:44:37,634 INFO] Step 10300/13200; acc: 91.9; ppl:   1.3; xent: 0.3; lr: 1.00000; sents:    2404; bsz:  447/ 621/48; 5733/7966 tok/s;    188 sec;
[2024-03-11 13:44:41,395 INFO] Step 10350/13200; acc: 91.9; ppl:   1.3; xent: 0.3; lr: 1.00000; sents:    2298; bsz:  442/ 605/46; 5880/8051 tok/s;    192 sec;
[2024-03-11 13:44:44,845 INFO] Step 10400/13200; acc: 92.4; ppl:   1.3; xent: 0.3; lr: 1.00000; sents:    2401; bsz:  458/ 631/48; 6636/9141 tok/s;    195 sec;
[2024-03-11 13:44:48,482 INFO] valid stats calculation
                           took: 3.6366543769836426 s.
[2024-03-11 13:44:48,482 INFO] Train perplexity: 3.44131
[2024-03-11 13:44:48,482 INFO] Train accuracy: 73.3112
[2024-03-11 13:44:48,482 INFO] Sentences processed: 103929
[2024-03-11 13:44:48,482 INFO] Average bsz:  449/ 620/47
[2024-03-11 13:44:48,482 INFO] Validation perplexity: 207.989
[2024-03-11 13:44:48,482 INFO] Validation accuracy: 40.5666
[2024-03-11 13:44:48,484 INFO] Saving checkpoint gt-finetune/gt-finetune_8200_01_step_10400.pt
[2024-03-11 13:44:52,354 INFO] Step 10450/13200; acc: 92.5; ppl:   1.3; xent: 0.3; lr: 1.00000; sents:    2399; bsz:  441/ 617/48; 2936/4106 tok/s;    203 sec;
[2024-03-11 13:44:56,258 INFO] Step 10500/13200; acc: 92.7; ppl:   1.3; xent: 0.3; lr: 1.00000; sents:    2299; bsz:  450/ 614/46; 5765/7870 tok/s;    207 sec;
[2024-03-11 13:45:00,022 INFO] Step 10550/13200; acc: 93.2; ppl:   1.3; xent: 0.2; lr: 1.00000; sents:    2393; bsz:  448/ 621/48; 5957/8254 tok/s;    210 sec;
[2024-03-11 13:45:03,456 INFO] Step 10600/13200; acc: 93.0; ppl:   1.3; xent: 0.2; lr: 1.00000; sents:    2393; bsz:  471/ 638/48; 6865/9290 tok/s;    214 sec;
[2024-03-11 13:45:07,070 INFO] valid stats calculation
                           took: 3.613171100616455 s.
[2024-03-11 13:45:07,070 INFO] Train perplexity: 3.16699
[2024-03-11 13:45:07,070 INFO] Train accuracy: 74.9463
[2024-03-11 13:45:07,070 INFO] Sentences processed: 113413
[2024-03-11 13:45:07,070 INFO] Average bsz:  450/ 620/47
[2024-03-11 13:45:07,070 INFO] Validation perplexity: 218.046
[2024-03-11 13:45:07,070 INFO] Validation accuracy: 40.792
[2024-03-11 13:45:07,071 INFO] Saving checkpoint gt-finetune/gt-finetune_8200_01_step_10600.pt
[2024-03-11 13:45:11,160 INFO] Step 10650/13200; acc: 93.4; ppl:   1.2; xent: 0.2; lr: 1.00000; sents:    2290; bsz:  440/ 604/46; 2855/3921 tok/s;    222 sec;
[2024-03-11 13:45:14,928 INFO] Step 10700/13200; acc: 93.5; ppl:   1.2; xent: 0.2; lr: 1.00000; sents:    2391; bsz:  438/ 612/48; 5817/8118 tok/s;    225 sec;
[2024-03-11 13:45:18,837 INFO] Step 10750/13200; acc: 93.8; ppl:   1.2; xent: 0.2; lr: 1.00000; sents:    2391; bsz:  450/ 624/48; 5759/7982 tok/s;    229 sec;
[2024-03-11 13:45:22,067 INFO] Step 10800/13200; acc: 93.8; ppl:   1.2; xent: 0.2; lr: 1.00000; sents:    2400; bsz:  446/ 622/48; 6899/9625 tok/s;    232 sec;
[2024-03-11 13:45:25,643 INFO] valid stats calculation
                           took: 3.5756659507751465 s.
[2024-03-11 13:45:25,643 INFO] Train perplexity: 2.94866
[2024-03-11 13:45:25,643 INFO] Train accuracy: 76.3718
[2024-03-11 13:45:25,643 INFO] Sentences processed: 122885
[2024-03-11 13:45:25,643 INFO] Average bsz:  449/ 620/47
[2024-03-11 13:45:25,643 INFO] Validation perplexity: 226.058
[2024-03-11 13:45:25,643 INFO] Validation accuracy: 41.6613
[2024-03-11 13:45:25,645 INFO] Saving checkpoint gt-finetune/gt-finetune_8200_01_step_10800.pt
[2024-03-11 13:45:29,756 INFO] Step 10850/13200; acc: 93.8; ppl:   1.2; xent: 0.2; lr: 1.00000; sents:    2399; bsz:  459/ 634/48; 2982/4122 tok/s;    240 sec;
[2024-03-11 13:45:33,547 INFO] Step 10900/13200; acc: 93.9; ppl:   1.2; xent: 0.2; lr: 1.00000; sents:    2303; bsz:  449/ 607/46; 5918/8012 tok/s;    244 sec;
[2024-03-11 13:45:37,359 INFO] Step 10950/13200; acc: 94.0; ppl:   1.2; xent: 0.2; lr: 1.00000; sents:    2307; bsz:  444/ 612/46; 5824/8027 tok/s;    248 sec;
[2024-03-11 13:45:40,444 INFO] Step 11000/13200; acc: 94.0; ppl:   1.2; xent: 0.2; lr: 1.00000; sents:    2401; bsz:  452/ 629/48; 7332/10190 tok/s;    251 sec;
[2024-03-11 13:45:44,018 INFO] valid stats calculation
                           took: 3.5739712715148926 s.
[2024-03-11 13:45:44,019 INFO] Train perplexity: 2.76998
[2024-03-11 13:45:44,019 INFO] Train accuracy: 77.6276
[2024-03-11 13:45:44,019 INFO] Sentences processed: 132295
[2024-03-11 13:45:44,019 INFO] Average bsz:  449/ 620/47
[2024-03-11 13:45:44,019 INFO] Validation perplexity: 271.729
[2024-03-11 13:45:44,019 INFO] Validation accuracy: 39.8583
[2024-03-11 13:45:44,020 INFO] Saving checkpoint gt-finetune/gt-finetune_8200_01_step_11000.pt
[2024-03-11 13:45:47,994 INFO] Step 11050/13200; acc: 94.6; ppl:   1.2; xent: 0.2; lr: 1.00000; sents:    2399; bsz:  444/ 618/48; 2941/4094 tok/s;    258 sec;
[2024-03-11 13:45:51,828 INFO] Step 11100/13200; acc: 94.4; ppl:   1.2; xent: 0.2; lr: 1.00000; sents:    2299; bsz:  448/ 613/46; 5843/7992 tok/s;    262 sec;
[2024-03-11 13:45:55,613 INFO] Step 11150/13200; acc: 94.6; ppl:   1.2; xent: 0.2; lr: 1.00000; sents:    2489; bsz:  457/ 642/50; 6040/8477 tok/s;    266 sec;
[2024-03-11 13:45:58,582 INFO] Step 11200/13200; acc: 94.6; ppl:   1.2; xent: 0.2; lr: 1.00000; sents:    2296; bsz:  447/ 611/46; 7535/10293 tok/s;    269 sec;
[2024-03-11 13:46:02,243 INFO] valid stats calculation
                           took: 3.6600873470306396 s.
[2024-03-11 13:46:02,244 INFO] Train perplexity: 2.62003
[2024-03-11 13:46:02,244 INFO] Train accuracy: 78.7575
[2024-03-11 13:46:02,244 INFO] Sentences processed: 141778
[2024-03-11 13:46:02,244 INFO] Average bsz:  449/ 620/47
[2024-03-11 13:46:02,244 INFO] Validation perplexity: 312.454
[2024-03-11 13:46:02,244 INFO] Validation accuracy: 40.8564
[2024-03-11 13:46:02,245 INFO] Saving checkpoint gt-finetune/gt-finetune_8200_01_step_11200.pt
[2024-03-11 13:46:06,165 INFO] Step 11250/13200; acc: 94.7; ppl:   1.2; xent: 0.2; lr: 1.00000; sents:    2391; bsz:  442/ 615/48; 2918/4056 tok/s;    277 sec;
[2024-03-11 13:46:10,066 INFO] Step 11300/13200; acc: 94.7; ppl:   1.2; xent: 0.2; lr: 1.00000; sents:    2293; bsz:  442/ 610/46; 5660/7813 tok/s;    280 sec;
[2024-03-11 13:46:14,044 INFO] Step 11350/13200; acc: 95.2; ppl:   1.2; xent: 0.2; lr: 1.00000; sents:    2392; bsz:  455/ 631/48; 5715/7926 tok/s;    284 sec;
[2024-03-11 13:46:16,765 INFO] Step 11400/13200; acc: 94.8; ppl:   1.2; xent: 0.2; lr: 1.00000; sents:    2304; bsz:  446/ 609/46; 8194/11202 tok/s;    287 sec;
[2024-03-11 13:46:20,445 INFO] valid stats calculation
                           took: 3.679588794708252 s.
[2024-03-11 13:46:20,445 INFO] Train perplexity: 2.49514
[2024-03-11 13:46:20,445 INFO] Train accuracy: 79.759
[2024-03-11 13:46:20,445 INFO] Sentences processed: 151158
[2024-03-11 13:46:20,445 INFO] Average bsz:  449/ 620/47
[2024-03-11 13:46:20,446 INFO] Validation perplexity: 270.951
[2024-03-11 13:46:20,446 INFO] Validation accuracy: 41.2428
[2024-03-11 13:46:20,447 INFO] Saving checkpoint gt-finetune/gt-finetune_8200_01_step_11400.pt
[2024-03-11 13:46:24,558 INFO] Step 11450/13200; acc: 95.1; ppl:   1.2; xent: 0.2; lr: 1.00000; sents:    2399; bsz:  445/ 621/48; 2855/3983 tok/s;    295 sec;
[2024-03-11 13:46:28,412 INFO] Step 11500/13200; acc: 95.0; ppl:   1.2; xent: 0.2; lr: 1.00000; sents:    2401; bsz:  458/ 632/48; 5949/8203 tok/s;    299 sec;
[2024-03-11 13:46:32,002 INFO] Step 11550/13200; acc: 95.2; ppl:   1.2; xent: 0.2; lr: 1.00000; sents:    2403; bsz:  446/ 623/48; 6206/8682 tok/s;    302 sec;
[2024-03-11 13:46:34,587 INFO] Step 11600/13200; acc: 95.3; ppl:   1.2; xent: 0.2; lr: 1.00000; sents:    2301; bsz:  447/ 610/46; 8642/11808 tok/s;    305 sec;
[2024-03-11 13:46:38,126 INFO] valid stats calculation
                           took: 3.538449764251709 s.
[2024-03-11 13:46:38,126 INFO] Train perplexity: 2.3873
[2024-03-11 13:46:38,126 INFO] Train accuracy: 80.6673
[2024-03-11 13:46:38,126 INFO] Sentences processed: 160662
[2024-03-11 13:46:38,126 INFO] Average bsz:  449/ 620/47
[2024-03-11 13:46:38,126 INFO] Validation perplexity: 343.871
[2024-03-11 13:46:38,127 INFO] Validation accuracy: 41.7579
[2024-03-11 13:46:38,128 INFO] Saving checkpoint gt-finetune/gt-finetune_8200_01_step_11600.pt
[2024-03-11 13:46:42,197 INFO] Step 11650/13200; acc: 95.2; ppl:   1.2; xent: 0.2; lr: 1.00000; sents:    2402; bsz:  471/ 641/48; 3092/4211 tok/s;    313 sec;
[2024-03-11 13:46:46,027 INFO] Step 11700/13200; acc: 95.4; ppl:   1.2; xent: 0.2; lr: 1.00000; sents:    2394; bsz:  443/ 618/48; 5782/8066 tok/s;    316 sec;
[2024-03-11 13:46:49,482 INFO] Step 11750/13200; acc: 95.3; ppl:   1.2; xent: 0.2; lr: 1.00000; sents:    2281; bsz:  438/ 602/46; 6334/8706 tok/s;    320 sec;
[2024-03-11 13:46:52,090 INFO] Step 11800/13200; acc: 95.4; ppl:   1.2; xent: 0.2; lr: 1.00000; sents:    2392; bsz:  463/ 636/48; 8873/12197 tok/s;    322 sec;
[2024-03-11 13:46:55,653 INFO] valid stats calculation
                           took: 3.563091993331909 s.
[2024-03-11 13:46:55,654 INFO] Train perplexity: 2.29429
[2024-03-11 13:46:55,654 INFO] Train accuracy: 81.4863
[2024-03-11 13:46:55,654 INFO] Sentences processed: 170131
[2024-03-11 13:46:55,654 INFO] Average bsz:  449/ 620/47
[2024-03-11 13:46:55,654 INFO] Validation perplexity: 367.738
[2024-03-11 13:46:55,654 INFO] Validation accuracy: 41.1462
[2024-03-11 13:46:55,655 INFO] Saving checkpoint gt-finetune/gt-finetune_8200_01_step_11800.pt
[2024-03-11 13:46:59,667 INFO] Step 11850/13200; acc: 95.5; ppl:   1.2; xent: 0.2; lr: 1.00000; sents:    2278; bsz:  437/ 599/46; 2884/3955 tok/s;    330 sec;
[2024-03-11 13:47:03,635 INFO] Step 11900/13200; acc: 95.6; ppl:   1.2; xent: 0.2; lr: 1.00000; sents:    2391; bsz:  455/ 628/48; 5732/7918 tok/s;    334 sec;
[2024-03-11 13:47:06,763 INFO] Step 11950/13200; acc: 95.5; ppl:   1.2; xent: 0.2; lr: 1.00000; sents:    2399; bsz:  445/ 619/48; 7116/9898 tok/s;    337 sec;
[2024-03-11 13:47:09,672 INFO] Step 12000/13200; acc: 95.8; ppl:   1.2; xent: 0.1; lr: 1.00000; sents:    2404; bsz:  449/ 624/48; 7713/10729 tok/s;    340 sec;
[2024-03-11 13:47:13,106 INFO] valid stats calculation
                           took: 3.433603048324585 s.
[2024-03-11 13:47:13,107 INFO] Train perplexity: 2.21393
[2024-03-11 13:47:13,107 INFO] Train accuracy: 82.226
[2024-03-11 13:47:13,107 INFO] Sentences processed: 179603
[2024-03-11 13:47:13,107 INFO] Average bsz:  449/ 620/47
[2024-03-11 13:47:13,107 INFO] Validation perplexity: 341.774
[2024-03-11 13:47:13,107 INFO] Validation accuracy: 41.4037
[2024-03-11 13:47:13,108 INFO] Saving checkpoint gt-finetune/gt-finetune_8200_01_step_12000.pt
[2024-03-11 13:47:17,140 INFO] Step 12050/13200; acc: 95.7; ppl:   1.2; xent: 0.1; lr: 1.00000; sents:    2298; bsz:  443/ 604/46; 2966/4046 tok/s;    348 sec;
[2024-03-11 13:47:20,976 INFO] Step 12100/13200; acc: 95.8; ppl:   1.2; xent: 0.1; lr: 1.00000; sents:    2404; bsz:  459/ 632/48; 5989/8245 tok/s;    351 sec;
[2024-03-11 13:47:23,761 INFO] Step 12150/13200; acc: 96.0; ppl:   1.1; xent: 0.1; lr: 1.00000; sents:    2401; bsz:  442/ 618/48; 7943/11093 tok/s;    354 sec;
[2024-03-11 13:47:26,919 INFO] Step 12200/13200; acc: 95.9; ppl:   1.2; xent: 0.1; lr: 1.00000; sents:    2302; bsz:  449/ 614/46; 7116/9732 tok/s;    357 sec;
[2024-03-11 13:47:30,417 INFO] valid stats calculation
                           took: 3.4972643852233887 s.
[2024-03-11 13:47:30,417 INFO] Train perplexity: 2.14324
[2024-03-11 13:47:30,417 INFO] Train accuracy: 82.9037
[2024-03-11 13:47:30,417 INFO] Sentences processed: 189008
[2024-03-11 13:47:30,417 INFO] Average bsz:  449/ 620/47
[2024-03-11 13:47:30,417 INFO] Validation perplexity: 340.477
[2024-03-11 13:47:30,417 INFO] Validation accuracy: 40.5988
[2024-03-11 13:47:30,418 INFO] Saving checkpoint gt-finetune/gt-finetune_8200_01_step_12200.pt
[2024-03-11 13:47:34,517 INFO] Step 12250/13200; acc: 95.6; ppl:   1.2; xent: 0.2; lr: 1.00000; sents:    2396; bsz:  471/ 638/48; 3100/4197 tok/s;    365 sec;
[2024-03-11 13:47:38,355 INFO] Step 12300/13200; acc: 95.7; ppl:   1.2; xent: 0.2; lr: 1.00000; sents:    2393; bsz:  443/ 616/48; 5768/8024 tok/s;    369 sec;
[2024-03-11 13:47:41,045 INFO] Step 12350/13200; acc: 96.0; ppl:   1.2; xent: 0.1; lr: 1.00000; sents:    2292; bsz:  449/ 617/46; 8351/11466 tok/s;    371 sec;
[2024-03-11 13:47:44,245 INFO] Step 12400/13200; acc: 96.1; ppl:   1.1; xent: 0.1; lr: 1.00000; sents:    2391; bsz:  441/ 611/48; 6885/9557 tok/s;    375 sec;
[2024-03-11 13:47:47,771 INFO] valid stats calculation
                           took: 3.525395393371582 s.
[2024-03-11 13:47:47,771 INFO] Train perplexity: 2.081
[2024-03-11 13:47:47,771 INFO] Train accuracy: 83.5202
[2024-03-11 13:47:47,771 INFO] Sentences processed: 198480
[2024-03-11 13:47:47,771 INFO] Average bsz:  449/ 620/47
[2024-03-11 13:47:47,771 INFO] Validation perplexity: 357.431
[2024-03-11 13:47:47,771 INFO] Validation accuracy: 42.112
[2024-03-11 13:47:47,773 INFO] Saving checkpoint gt-finetune/gt-finetune_8200_01_step_12400.pt
[2024-03-11 13:47:51,800 INFO] Step 12450/13200; acc: 96.1; ppl:   1.1; xent: 0.1; lr: 1.00000; sents:    2294; bsz:  445/ 603/46; 2947/3992 tok/s;    382 sec;
[2024-03-11 13:47:55,580 INFO] Step 12500/13200; acc: 95.9; ppl:   1.1; xent: 0.1; lr: 1.00000; sents:    2391; bsz:  459/ 633/48; 6067/8375 tok/s;    386 sec;
[2024-03-11 13:47:58,107 INFO] Step 12550/13200; acc: 96.3; ppl:   1.1; xent: 0.1; lr: 1.00000; sents:    2400; bsz:  445/ 621/48; 8809/12297 tok/s;    388 sec;
[2024-03-11 13:48:01,749 INFO] Step 12600/13200; acc: 96.2; ppl:   1.1; xent: 0.1; lr: 1.00000; sents:    2399; bsz:  457/ 635/48; 6276/8718 tok/s;    392 sec;
[2024-03-11 13:48:05,252 INFO] valid stats calculation
                           took: 3.5022051334381104 s.
[2024-03-11 13:48:05,252 INFO] Train perplexity: 2.02489
[2024-03-11 13:48:05,252 INFO] Train accuracy: 84.096
[2024-03-11 13:48:05,253 INFO] Sentences processed: 207964
[2024-03-11 13:48:05,253 INFO] Average bsz:  449/ 620/47
[2024-03-11 13:48:05,253 INFO] Validation perplexity: 371.851
[2024-03-11 13:48:05,253 INFO] Validation accuracy: 42.4018
[2024-03-11 13:48:05,254 INFO] Saving checkpoint gt-finetune/gt-finetune_8200_01_step_12600.pt
[2024-03-11 13:48:09,368 INFO] Step 12650/13200; acc: 96.2; ppl:   1.1; xent: 0.1; lr: 1.00000; sents:    2304; bsz:  433/ 597/46; 2840/3916 tok/s;    400 sec;
[2024-03-11 13:48:13,025 INFO] Step 12700/13200; acc: 96.2; ppl:   1.1; xent: 0.1; lr: 1.00000; sents:    2403; bsz:  451/ 626/48; 6164/8564 tok/s;    403 sec;
[2024-03-11 13:48:15,575 INFO] Step 12750/13200; acc: 96.1; ppl:   1.1; xent: 0.1; lr: 1.00000; sents:    2305; bsz:  448/ 614/46; 8791/12037 tok/s;    406 sec;
[2024-03-11 13:48:19,290 INFO] Step 12800/13200; acc: 96.3; ppl:   1.1; xent: 0.1; lr: 1.00000; sents:    2495; bsz:  458/ 644/50; 6167/8676 tok/s;    410 sec;
[2024-03-11 13:48:22,787 INFO] valid stats calculation
                           took: 3.4966464042663574 s.
[2024-03-11 13:48:22,787 INFO] Train perplexity: 1.97484
[2024-03-11 13:48:22,787 INFO] Train accuracy: 84.622
[2024-03-11 13:48:22,787 INFO] Sentences processed: 217471
[2024-03-11 13:48:22,787 INFO] Average bsz:  449/ 620/47
[2024-03-11 13:48:22,787 INFO] Validation perplexity: 393.763
[2024-03-11 13:48:22,787 INFO] Validation accuracy: 41.5969
[2024-03-11 13:48:22,789 INFO] Saving checkpoint gt-finetune/gt-finetune_8200_01_step_12800.pt
[2024-03-11 13:48:26,860 INFO] Step 12850/13200; acc: 96.1; ppl:   1.1; xent: 0.1; lr: 1.00000; sents:    2299; bsz:  448/ 612/46; 2959/4042 tok/s;    417 sec;
[2024-03-11 13:48:30,213 INFO] Step 12900/13200; acc: 96.3; ppl:   1.1; xent: 0.1; lr: 1.00000; sents:    2393; bsz:  454/ 629/48; 6776/9386 tok/s;    421 sec;
[2024-03-11 13:48:32,857 INFO] Step 12950/13200; acc: 96.4; ppl:   1.1; xent: 0.1; lr: 1.00000; sents:    2294; bsz:  430/ 596/46; 8142/11272 tok/s;    423 sec;
[2024-03-11 13:48:36,767 INFO] Step 13000/13200; acc: 96.4; ppl:   1.1; xent: 0.1; lr: 1.00000; sents:    2391; bsz:  454/ 630/48; 5813/8060 tok/s;    427 sec;
[2024-03-11 13:48:40,254 INFO] valid stats calculation
                           took: 3.486999273300171 s.
[2024-03-11 13:48:40,254 INFO] Train perplexity: 1.93007
[2024-03-11 13:48:40,254 INFO] Train accuracy: 85.1058
[2024-03-11 13:48:40,254 INFO] Sentences processed: 226848
[2024-03-11 13:48:40,254 INFO] Average bsz:  449/ 620/47
[2024-03-11 13:48:40,255 INFO] Validation perplexity: 427.859
[2024-03-11 13:48:40,255 INFO] Validation accuracy: 40.9208
[2024-03-11 13:48:40,256 INFO] Saving checkpoint gt-finetune/gt-finetune_8200_01_step_13000.pt
[2024-03-11 13:48:44,289 INFO] Step 13050/13200; acc: 96.4; ppl:   1.1; xent: 0.1; lr: 1.00000; sents:    2391; bsz:  445/ 620/48; 2958/4121 tok/s;    435 sec;
[2024-03-11 13:48:47,354 INFO] Step 13100/13200; acc: 96.5; ppl:   1.1; xent: 0.1; lr: 1.00000; sents:    2392; bsz:  460/ 631/48; 7499/10302 tok/s;    438 sec;
[2024-03-11 13:48:50,388 INFO] Step 13150/13200; acc: 96.2; ppl:   1.1; xent: 0.1; lr: 1.00000; sents:    2304; bsz:  444/ 608/46; 7324/10022 tok/s;    441 sec;
[2024-03-11 13:48:54,287 INFO] Step 13200/13200; acc: 96.6; ppl:   1.1; xent: 0.1; lr: 1.00000; sents:    2293; bsz:  443/ 608/46; 5682/7801 tok/s;    445 sec;
[2024-03-11 13:48:57,821 INFO] valid stats calculation
                           took: 3.5337107181549072 s.
[2024-03-11 13:48:57,822 INFO] Train perplexity: 1.88933
[2024-03-11 13:48:57,822 INFO] Train accuracy: 85.5571
[2024-03-11 13:48:57,822 INFO] Sentences processed: 236228
[2024-03-11 13:48:57,822 INFO] Average bsz:  449/ 620/47
[2024-03-11 13:48:57,822 INFO] Validation perplexity: 430.285
[2024-03-11 13:48:57,822 INFO] Validation accuracy: 42.5306
[2024-03-11 13:48:57,823 INFO] Saving checkpoint gt-finetune/gt-finetune_8200_01_step_13200.pt
