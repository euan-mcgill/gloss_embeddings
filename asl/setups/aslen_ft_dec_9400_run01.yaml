## Where the samples will be written
save_data: gt-pt-dec
## Where the vocab(s) will be written
src_vocab: gt-pt-dec/gt-pt-dec.vocab.src
tgt_vocab: gt-pt-dec/gt-pt-dec.vocab.tgt
# src_feats_vocab: 
#     feat_0:  toy-es-lse/run_dep/wd.vocab.feat_0
#     feat_1:  toy-es-lse/run_pos/wd.vocab.feat_1
feat_merge: "concat"
# Prevent overwriting existing files in the folder
overwrite: True
bucket_size: 144
# Corpus opts:
data:
    corpus_1:
        path_src: asl_train.txt
        path_tgt: en_train_tok.txt
        transforms: [inferfeats]
    valid:
        path_src: asl_dev.txt
        path_tgt: en_dev_tok.txt
        transforms: [inferfeats]
# Train on a single GPU
world_size: 1
gpu_ranks: [0]

# Where to save the checkpoints
save_model: gt-ft-dec/gt-ft-dec_9400_01
save_checkpoint_steps: 200
train_steps: 14400
valid_steps: 200

#both_embeddings: glove_dir/glove-sbwc.i25.vec
# to set src and tgt embeddings separately:
#src_embeddings: asl_w2v.txt
tgt_embeddings: en_w2v.txt

# supported types: GloVe, word2vec
embeddings_type: "word2vec"

# word_vec_size need to match with the pretrained embeddings dimensions
word_vec_size: 300
