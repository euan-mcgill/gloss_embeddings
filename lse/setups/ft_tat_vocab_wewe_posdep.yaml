# Finetune best model from run 1

## Where the samples will be written
save_data: tg-pretrain
## Where the vocab(s) will be written
src_vocab: tg-pretrain/tg-pretrain.vocab.src
tgt_vocab: tg-pretrain/tg-pretrain.vocab.tgt
# src_feats_vocab: 
#     feat_0:  toy-es-lse/run_dep/wd.vocab.feat_0
#     feat_1:  toy-es-lse/run_pos/wd.vocab.feat_1
feat_merge: "concat"
# Prevent overwriting existing files in the folder
overwrite: True

# Corpus opts:
data:
    corpus_1:
        path_src: vocab_ft_tat_train_es_posdep_tagged.txt
        path_tgt: vocab_ft_tat_train_gloss.txt
        transforms: [inferfeats]
    valid:
        path_src: eslse_dev_es_tok.txt
        path_tgt: eslse_dev_gloss.txt
        transforms: [inferfeats]
# Train on a single GPU
world_size: 1
gpu_ranks: [0]

# Transform options
reversible_tokenization: "joiner"
n_src_feats: 2
src_feats_defaults: "Xï¿¨X"

# Where to save the checkpoints - for finetuning must specify # of steps from final checkpoint!
save_model: tg-pretrain/tg-pretrain
save_checkpoint_steps: 200
train_steps: 10000
valid_steps: 200

#both_embeddings: glove_dir/glove-sbwc.i25.vec
# to set src and tgt embeddings separately:
src_embeddings: sbw_vectors_w2v.txt
tgt_embeddings: lse_w2v.txt

# supported types: GloVe, word2vec
embeddings_type: "word2vec"

# word_vec_size need to match with the pretrained embeddings dimensions
word_vec_size: 300
